{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cZCM65CBt1CJ"
   },
   "source": [
    "##### Copyright 2019 The TensorFlow Authors.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2022-08-09T02:10:11.391957Z",
     "iopub.status.busy": "2022-08-09T02:10:11.391697Z",
     "iopub.status.idle": "2022-08-09T02:10:11.396056Z",
     "shell.execute_reply": "2022-08-09T02:10:11.395458Z"
    },
    "id": "JOgMcEajtkmg"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rCSP-dbMw88x"
   },
   "source": [
    "# 画像セグメンテーション"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NEWs8JXRuGex"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td><a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/images/segmentation\">     <img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">     TensorFlow.org で表示</a></td>\n",
    "  <td><a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/ja/tutorials/images/segmentation.ipynb\">     <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">     Google Colab で実行</a></td>\n",
    "  <td><a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/ja/tutorials/images/segmentation.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">GitHub でソースを表示</a></td>\n",
    "  <td><a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/ja/tutorials/images/segmentation.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">ノートブックをダウンロード</a></td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sMP7mglMuGT2"
   },
   "source": [
    "このチュートリアルでは、修正した <a href=\"https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/\" class=\"external\">U-Net</a> を使用した画像セグメンテーションのタスクに焦点を当てます。\n",
    "\n",
    "## 画像セグメンテーションとは\n",
    "\n",
    "画像分類タスクでは、ネットワークが各入力画像にラベル（またはクラス）を割り当てますが、そのオブジェクトの形状やどのピクセルがどのオブジェクトに属しているかなどを知りたい場合はどうすればよいでしょうか。この場合、画像のピクセルごとにクラスを割り当てようと考えるでしょう。このタスクはセグメンテーションとして知られています。セグメンテーションモデルは、画像に関してはるかに詳細な情報を返します。画像セグメンテーションには、医用イメージング、自動走行車、衛星撮像など、数多くの用途があります。\n",
    "\n",
    "このチュートリアルでは [Oxford-IIIT Pet Dataset](https://www.robots.ox.ac.uk/~vgg/data/pets/)（Parkhi <em>et al</em>）を使用します。データセットには、37 種のペット品種と、品種当たり 200 枚の画像（train と test split で約 100 枚ずつ）が含まれます。それぞれの画像には対応するラベルとピクセル方向のマスクが含まれます。マスクは各ピクセルのクラスラベルです。各ピクセルには、次のいずれかのカテゴリが指定されます。\n",
    "\n",
    "- クラス 1 : ペットに属するピクセル。\n",
    "- クラス 2 : ペットと境界のピクセル。\n",
    "- クラス 3: 上記のいずれにも該当しない、または周囲のピクセル。"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!ls -al\n",
    "!pwd\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tOajUHWm3CIm",
    "outputId": "f6e746c4-16cb-4b24-e019-914d58e0e32b"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-09T02:10:16.465132Z",
     "iopub.status.busy": "2022-08-09T02:10:16.464355Z",
     "iopub.status.idle": "2022-08-09T02:10:18.796177Z",
     "shell.execute_reply": "2022-08-09T02:10:18.795394Z"
    },
    "id": "YQX7R4bhZy5h"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from keras.layers import Conv2D, BatchNormalization, Activation, UpSampling2D, \\\n",
    "    AveragePooling2D\n",
    "from keras.layers import GlobalAveragePooling2D, Concatenate, Input, Reshape\n",
    "from classification_models.keras import Classifiers\n",
    "from keras.models import Model\n",
    "from keras.applications.resnet import ResNet50\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# tf のバージョンを確認\n",
    "print(tf.__version__)\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "device_lib.list_local_devices()"
   ],
   "metadata": {
    "id": "tVH3_6He2cSI",
    "outputId": "f2bbbe6f-bfea-4280-afa3-ed44201f7da2",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rJcVdj_U4vzf"
   },
   "source": [
    "また、画像の色値は `[0,1]` の範囲に正規化されています。最後に、上記で説明したとおり、セグメンテーションのマスクは {1, 2, 3} のいずれかでラベル付けされています。便宜上、セグメンテーションマスクから 1 を減算して、ラベルを {0, 1, 2} としましょう。"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 画像のラベル一覧\n",
    "\n",
    "class_names = ['Background',\n",
    "               'Holds',\n",
    "               'Volume',\n",
    "               'Mat',\n",
    "               'Human',\n",
    "               ]\n",
    "\n",
    "# 画像の色を定義\n",
    "class_colors = [\n",
    "    [255, 0, 0],\n",
    "    [0, 128, 0],\n",
    "    [0, 0, 255],\n",
    "    [152, 251, 152],\n",
    "    [255, 255, 0]\n",
    "]\n",
    "\n",
    "clas_labels = [0, 1, 1, 2, 3]\n",
    "\n",
    "# Volume も Holds も同じ色にする\n",
    "OUTPUT_CLASSES = 4\n",
    "\n",
    "os.getcwd()\n",
    "input_file_path = os.getcwd() + '/../../HoldsSegmentationDatasets/input'\n",
    "segment_file_path = os.getcwd() + '/../../HoldsSegmentationDatasets/segment'\n",
    "\n",
    "print(input_file_path)\n",
    "print(segment_file_path)\n",
    "\n",
    "input_file_list = glob.glob(input_file_path + '/*')\n",
    "segment_file_list = glob.glob(segment_file_path + '/*')\n",
    "\n",
    "# ファイル名でソート\n",
    "input_file_list.sort()\n",
    "segment_file_list.sort()\n",
    "\n",
    "IMAGE_WIDTH = 128 * 4\n",
    "IMAGE_HEIGHT = 128 * 4"
   ],
   "metadata": {
    "id": "asawNdst2cSI",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "45739cea-291a-4551-dec4-376819f3f3ce"
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def show_img(img):\n",
    "    plt.imshow(tf.keras.utils.array_to_img(img))\n",
    "    plt.axis('off')"
   ],
   "metadata": {
    "id": "3_O-Vh382cSI"
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "65-qHTjX5VZh"
   },
   "source": [
    "データセットにはすでに必要となる training と test split が含まれているため、そのまま同じ split を使用します。"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "id": "R-jia7IP2cSI"
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def load_input(file_path):\n",
    "    # png を読み込む\n",
    "    png_img = tf.io.read_file(file_path)\n",
    "    input_image = tf.image.decode_png(png_img, channels=3)\n",
    "\n",
    "    # 画像をリサイズ\n",
    "    input_image = tf.image.resize(input_image, (IMAGE_WIDTH, IMAGE_HEIGHT))\n",
    "\n",
    "    # 画像を正規化\n",
    "    input_image = tf.cast(input_image, tf.float32) / 255.0\n",
    "\n",
    "    return input_image"
   ],
   "metadata": {
    "id": "3KviHsp-2cSI"
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def load_segment(file_path):\n",
    "    # png を numpyとして読み込む\n",
    "    png_img = np.array(\n",
    "        tf.image.decode_png(tf.io.read_file(file_path), channels=3))\n",
    "\n",
    "    # 新しい形状の配列を初期化\n",
    "    class_indices = np.zeros((png_img.shape[0], png_img.shape[1], 1), dtype=int)\n",
    "\n",
    "    # 各クラスカラーに対してループ\n",
    "    for index, color in enumerate(class_colors):\n",
    "        # クラスカラーに一致するピクセルを検索し、インデックスで置換\n",
    "        class_indices[(png_img == color).all(axis=-1)] = clas_labels[index]\n",
    "\n",
    "    # tf に変換\n",
    "    class_indices = tf.convert_to_tensor(class_indices, dtype=tf.int64)\n",
    "\n",
    "    # 画像をリサイズ, リサイズするとき補間を行わないように設定\n",
    "    class_indices = tf.image.resize(class_indices, (IMAGE_WIDTH, IMAGE_HEIGHT),\n",
    "                                    method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "\n",
    "    return class_indices\n"
   ],
   "metadata": {
    "id": "HNaPEE8g2cSI"
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def augment(in_image: list, in_labels: list):\n",
    "    \"\"\"\n",
    "    入力画像をもとにデータをかさ増しする.\n",
    "    :param in_labels: \n",
    "    :param in_image: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    output_image = []\n",
    "    output_labels = []\n",
    "    count = 0\n",
    "    for image, label in zip(in_image, in_labels):\n",
    "        count += 1\n",
    "        # print(\"augment: \", count)\n",
    "\n",
    "        # 通常の画像\n",
    "        output_image.append(image)\n",
    "        output_labels.append(label)\n",
    "\n",
    "        # 左右反転\n",
    "        output_image.append(tf.image.flip_left_right(image))\n",
    "        output_labels.append(tf.image.flip_left_right(label))\n",
    "\n",
    "        # 上下反転\n",
    "        output_image.append(tf.image.flip_up_down(image))\n",
    "        output_labels.append(tf.image.flip_up_down(label))\n",
    "\n",
    "        # 90度回転\n",
    "        output_image.append(tf.image.rot90(image))\n",
    "        output_labels.append(tf.image.rot90(label))\n",
    "\n",
    "        # 180度回転\n",
    "        output_image.append(tf.image.rot90(image, k=2))\n",
    "        output_labels.append(tf.image.rot90(label, k=2))\n",
    "\n",
    "        # 270度回転\n",
    "        output_image.append(tf.image.rot90(image, k=3))\n",
    "        output_labels.append(tf.image.rot90(label, k=3))\n",
    "\n",
    "        # 輝度を変更 (0.1)\n",
    "        output_image.append(tf.image.adjust_brightness(image, 0.1))\n",
    "        output_labels.append(label)\n",
    "\n",
    "        # 輝度を0.\n",
    "        # 輝度を変更 (0.2)\n",
    "        output_image.append(tf.image.adjust_brightness(image, 0.2))\n",
    "        output_labels.append(label)\n",
    "\n",
    "        # 彩度を変更 (0.1)\n",
    "        output_image.append(tf.image.adjust_saturation(image, 0.1))\n",
    "        output_labels.append(label)\n",
    "\n",
    "        # 彩度を変更 (0.2)\n",
    "        output_image.append(tf.image.adjust_saturation(image, 0.2))\n",
    "        output_labels.append(label)\n",
    "\n",
    "        # コントラストを変更 (0.1)\n",
    "        output_image.append(tf.image.adjust_contrast(image, 0.1))\n",
    "        output_labels.append(label)\n",
    "\n",
    "        # 画像反転\n",
    "        output_image.append(tf.image.flip_left_right(image))\n",
    "        output_labels.append(tf.image.flip_left_right(label))\n",
    "\n",
    "        # ランダムなノイズの追加\n",
    "        noise = tf.random.normal(shape=tf.shape(image), mean=0.0, stddev=0.1)\n",
    "        output_image.append(tf.clip_by_value(image + noise, 0.0, 1.0))\n",
    "        output_labels.append(label)\n",
    "\n",
    "        crop_size_list = [0.5, 0.75, 0.9]\n",
    "        for crop_size in crop_size_list:\n",
    "            # トリミング\n",
    "            crop_img = tf.image.central_crop(image, crop_size)\n",
    "            crop_label = tf.image.central_crop(label, crop_size)\n",
    "\n",
    "            # 画像のサイズを元に戻す\n",
    "            # 補間はかけない\n",
    "            crop_img = tf.image.resize(crop_img, (IMAGE_WIDTH, IMAGE_HEIGHT),\n",
    "                                       method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "            crop_label = tf.image.resize(crop_label,\n",
    "                                         (IMAGE_WIDTH, IMAGE_HEIGHT),\n",
    "                                         method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "\n",
    "            output_image.append(crop_img)\n",
    "            output_labels.append(crop_label)\n",
    "\n",
    "    return output_image, output_labels"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 一度すべての画像を読み込んだあとテストとトレーニングに分ける\n",
    "input_images = []\n",
    "segment_images = []\n",
    "for input_file, segment_file in zip(input_file_list, segment_file_list):\n",
    "    input_images.append(load_input(input_file))\n",
    "    segment_images.append(load_segment(segment_file))\n",
    "\n",
    "image_set = []\n",
    "for input, segment in zip(input_images, segment_images):\n",
    "    image_set.append((input, segment))\n",
    "\n",
    "# ランダムに画像を並び替える\n",
    "np.random.shuffle(image_set)\n",
    "\n",
    "input_images = []\n",
    "segment_images = []\n",
    "for input, segment in image_set:\n",
    "    input_images.append(input)\n",
    "    segment_images.append(segment)\n",
    "\n",
    "# segment_image に含まれるラベルの数を確認\n",
    "print(np.unique(segment_images))\n",
    "\n",
    "print(f\"input_images: {len(input_images)}\"\n",
    "      f\"\\nsegment_images: {len(segment_images)}\")\n",
    "\n",
    "train_image_size = int(len(input_images) * 0.9)\n",
    "val_image_size = int(len(input_images) * 0.08)\n",
    "test_image_size = len(input_images) - train_image_size - val_image_size\n",
    "\n",
    "print(f\"train_image_size: {train_image_size}\"\n",
    "      f\"\\nval_image_size: {val_image_size}\"\n",
    "      f\"\\ntest_image_size: {test_image_size}\")\n",
    "\n",
    "train_image_list = input_images[:train_image_size]\n",
    "val_image_list = input_images[\n",
    "                 train_image_size:train_image_size + val_image_size]\n",
    "test_image_list = input_images[train_image_size + val_image_size:]\n",
    "\n",
    "train_label_list = segment_images[:train_image_size]\n",
    "val_label_list = segment_images[\n",
    "                 train_image_size:train_image_size + val_image_size]\n",
    "test_label_list = segment_images[train_image_size + val_image_size:]\n",
    "\n",
    "# データのかさ増し\n",
    "train_image_list, train_label_list = augment(train_image_list,\n",
    "                                             train_label_list)\n",
    "\n",
    "# テストとトレーニングに分ける\n",
    "train_images_np = np.array(train_image_list)\n",
    "val_image_np = np.array(val_image_list)\n",
    "test_images_np = np.array(test_image_list)\n",
    "\n",
    "train_segments = np.array(train_label_list)\n",
    "val_segments = np.array(val_label_list)\n",
    "test_segments = np.array(test_label_list)\n",
    "\n",
    "print(f\"train_images_np: \", train_images_np.shape)\n",
    "print(f\"val_image_np: \", val_image_np.shape)\n",
    "print(f\"test_image_np: \", test_images_np.shape)\n",
    "\n",
    "# データセットに変換. 画像トラベルを一つにする\n",
    "train_images = tf.data.Dataset.from_tensor_slices(\n",
    "    (train_images_np, train_segments))\n",
    "val_images = tf.data.Dataset.from_tensor_slices(\n",
    "    (val_image_np, val_segments))\n",
    "test_images = tf.data.Dataset.from_tensor_slices(\n",
    "    (test_images_np, test_segments))\n",
    "\n",
    "TRAIN_LENGTH = len(train_images)\n",
    "BATCH_SIZE = 5\n",
    "BUFFER_SIZE = BATCH_SIZE\n",
    "STEPS_PER_EPOCH = TRAIN_LENGTH // BATCH_SIZE\n",
    "\n",
    "print(f\"TRAIN_LENGTH: {TRAIN_LENGTH}\"\n",
    "      f\"\\nBATCH_SIZE: {BATCH_SIZE}\"\n",
    "      f\"\\nBUFFER_SIZE: {BUFFER_SIZE}\"\n",
    "      f\"\\nSTEPS_PER_EPOCH: {STEPS_PER_EPOCH}\")\n",
    "print(f\"\\ntrain_images: {len(train_images)}\"\n",
    "      f\"\\nval_images: {len(val_images)}\",\n",
    "      f\"\\ntest_images: {len(test_images)}\")\n"
   ],
   "metadata": {
    "id": "qa01bSYe2cSI",
    "outputId": "f77c8e78-c5fc-46e7-a043-0a6cd1036cf2",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T9hGHyg8L3Y1"
   },
   "source": [
    "次のクラスは、画像をランダムにフリップする単純な拡張を実行します。詳細は、[画像のデータ拡張](data_augmentation.ipynb)チュートリアルをご覧ください。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-09T02:10:23.696326Z",
     "iopub.status.busy": "2022-08-09T02:10:23.695643Z",
     "iopub.status.idle": "2022-08-09T02:10:23.700499Z",
     "shell.execute_reply": "2022-08-09T02:10:23.699894Z"
    },
    "id": "fUWdDJRTL0PP"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class Augment(tf.keras.layers.Layer):\n",
    "    def __init__(self, seed=42):\n",
    "        super().__init__()\n",
    "        self.flip = tf.keras.layers.RandomFlip(\"horizontal\", seed=seed)\n",
    "        self.rot = tf.keras.layers.RandomRotation(0.1, seed=seed)\n",
    "\n",
    "    def call(self, inputs, labels):\n",
    "        return inputs, labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xTIbNIBdcgL3"
   },
   "source": [
    "入力パイプラインをビルドし、入力をバッチ処理した後に拡張を適用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-09T02:10:23.703658Z",
     "iopub.status.busy": "2022-08-09T02:10:23.703414Z",
     "iopub.status.idle": "2022-08-09T02:10:23.891329Z",
     "shell.execute_reply": "2022-08-09T02:10:23.890577Z"
    },
    "id": "VPscskQcNCx4"
   },
   "outputs": [],
   "source": [
    "train_batches = (\n",
    "    train_images\n",
    "    .cache()\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .repeat()\n",
    "    .map(Augment())\n",
    "    .prefetch(buffer_size=tf.data.AUTOTUNE))\n",
    "\n",
    "val_batches = val_images.batch(min(len(val_images), BATCH_SIZE))\n",
    "test_batches = test_images.batch(min(len(test_images), BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xa3gMAE_9qNa"
   },
   "source": [
    "画像サンプルと対応するデータセットのマスクを可視化しましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-09T02:10:23.896005Z",
     "iopub.status.busy": "2022-08-09T02:10:23.895096Z",
     "iopub.status.idle": "2022-08-09T02:10:23.900331Z",
     "shell.execute_reply": "2022-08-09T02:10:23.899689Z"
    },
    "id": "3N2RPAAW9q4W"
   },
   "outputs": [],
   "source": [
    "CLASS_COLORS = [\n",
    "    [255, 0, 0],\n",
    "    [0, 128, 0],\n",
    "    [152, 251, 152],\n",
    "    [255, 255, 0]\n",
    "]\n",
    "CLASS_NAME = ['Background',\n",
    "              'Holds',\n",
    "              'Mat',\n",
    "              'Human',\n",
    "              ]\n",
    "\n",
    "\n",
    "def create_colored_mask(mask):\n",
    "    \"\"\"\n",
    "    マスクに色を適用する関数\n",
    "    \"\"\"\n",
    "    colored_mask = np.zeros((*mask.shape, 3), dtype=np.uint8)\n",
    "    for i, color in enumerate(CLASS_COLORS):\n",
    "        colored_mask[mask == i] = np.array(color)\n",
    "    return colored_mask\n",
    "\n",
    "\n",
    "def display(display_list):\n",
    "    plt.figure(figsize=(15, 15))\n",
    "\n",
    "    title = ['Input Image', 'True Mask', 'Predicted Mask']\n",
    "\n",
    "    for i in range(len(display_list)):\n",
    "        plt.subplot(1, len(display_list), i + 1)\n",
    "        plt.title(title[i])\n",
    "\n",
    "        # マスクの場合、色を適用\n",
    "        if i > 0:  # Input Image は除外\n",
    "            display_image = create_colored_mask(display_list[i])\n",
    "            # 不要な軸が含まれている場合は削除しておく\n",
    "            if len(display_image.shape) > 3:\n",
    "                display_image = np.squeeze(display_image, axis=2)\n",
    "            display_image = tf.keras.utils.array_to_img(display_image)\n",
    "        else:\n",
    "            # 不要な軸が含まれている場合は削除しておく\n",
    "            if len(display_list[i].shape) > 3:\n",
    "                display_list[i] = np.squeeze(display_list[i], axis=3)\n",
    "            display_image = tf.keras.utils.array_to_img(display_list[i])\n",
    "\n",
    "        plt.imshow(display_image)\n",
    "        plt.axis('off')\n",
    "\n",
    "    # クラス名と色の凡例を表示\n",
    "    patches = [plt.plot([], [], marker=\"s\", ms=10, ls=\"\", mec=None, color=np.array(color) / 255,\n",
    "                        label=\"{:s}\".format(CLASS_NAME[i]))[0] for i, color in\n",
    "               enumerate(CLASS_COLORS)]\n",
    "    plt.legend(handles=patches, bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "for images, masks in train_batches.take(5):\n",
    "    sample_image, sample_mask = images[0], masks[0]\n",
    "\n",
    "    display([sample_image, sample_mask])"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FAOe93FRMk3w"
   },
   "source": [
    "## モデルを定義する\n",
    "\n",
    "ここで使用されるモデルは修正された [U-Net](https://arxiv.org/abs/1505.04597) です。U-Net には、エンコーダ（ダウンサンプラー）とデコーダ（アップサンプラー）が含まれます。強力な特徴量を理解してトレーニング可能なパラメータ数を減らすため、MobileNetV2 というトレーニング済みモデルをエンコーダとして使用します。デコーダについてはアップサンプルブロックを使用しますが、これは TensorFlow Examples リポジトリの [pix2pix](https://github.com/tensorflow/examples/blob/master/tensorflow_examples/models/pix2pix/pix2pix.py) の例に実装済みです。（ノートブックの [pix2pix: 条件付き GAN による画像から画像への変換](../generative/pix2pix.ipynb)チュートリアルをご覧ください。）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W4mQle3lthit"
   },
   "source": [
    "前述したように、エンコーダは事前トレーニング済み MobileNetV2 モデルであり、[tf.keras.applications](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/applications) で即座に使用できるように準備されています。エンコーダはモデル内の中間レイヤーからの特定の出力で構成されています。トレーニングプロセス中にエンコーダはトレーニングされないので注意してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j0DGH_4T0VYn"
   },
   "source": [
    "## モデルをトレーニングする\n",
    "\n",
    "では、後は、モデルををコンパイルしてトレーニングするだけです。\n",
    "\n",
    "これはマルチクラスの分類問題であり、ラベルがクラスごとのピクセルのスコアのベクトルではなくスカラー整数であるため、`tf.keras.losses.CategoricalCrossentropy` 損失関数を使用して、`from_logits` を `True` に設定します。\n",
    "\n",
    "推論を実行すると、ピクセルに割り当てられたラベルが最も高い値を持つチャンネルです。これは、`create_mask` 関数の作用です。"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import Model\n",
    "\n",
    "\n",
    "class UNet(Model):\n",
    "    def __init__(self, class_num):\n",
    "        super().__init__()\n",
    "        # Network\n",
    "        self.enc = Encoder(class_num)\n",
    "        self.dec = Decoder(class_num)\n",
    "\n",
    "    def call(self, x, **kwargs):\n",
    "        z1, z2, z3, z4_dropout, z5_dropout = self.enc(x)\n",
    "        y = self.dec(z1, z2, z3, z4_dropout, z5_dropout)\n",
    "\n",
    "        return y\n",
    "\n",
    "\n",
    "class Encoder(Model):\n",
    "    def __init__(self, class_num):\n",
    "        super().__init__()\n",
    "        # Network\n",
    "        self.block1_conv1 = tf.keras.layers.Conv2D(64, (3, 3),\n",
    "                                                   name='block1_conv1',\n",
    "                                                   activation='relu',\n",
    "                                                   padding='same',\n",
    "                                                   input_shape=(IMAGE_WIDTH,\n",
    "                                                                IMAGE_HEIGHT,\n",
    "                                                                3))\n",
    "        self.block1_conv2 = tf.keras.layers.Conv2D(64, (3, 3),\n",
    "                                                   name='block1_conv2',\n",
    "                                                   padding='same')\n",
    "        self.block1_bn = tf.keras.layers.BatchNormalization()\n",
    "        self.block1_act = tf.keras.layers.ReLU()\n",
    "        self.block1_pool = tf.keras.layers.MaxPooling2D((2, 2), strides=None,\n",
    "                                                        name='block1_pool')\n",
    "\n",
    "        self.block2_conv1 = tf.keras.layers.Conv2D(128, (3, 3),\n",
    "                                                   name='block2_conv1',\n",
    "                                                   activation='relu',\n",
    "                                                   padding='same')\n",
    "        self.block2_conv2 = tf.keras.layers.Conv2D(128, (3, 3),\n",
    "                                                   name='block2_conv2',\n",
    "                                                   padding='same')\n",
    "        self.block2_bn = tf.keras.layers.BatchNormalization()\n",
    "        self.block2_act = tf.keras.layers.ReLU()\n",
    "        self.block2_pool = tf.keras.layers.MaxPooling2D((2, 2), strides=None,\n",
    "                                                        name='block2_pool')\n",
    "\n",
    "        self.block3_conv1 = tf.keras.layers.Conv2D(256, (3, 3),\n",
    "                                                   name='block3_conv1',\n",
    "                                                   activation='relu',\n",
    "                                                   padding='same')\n",
    "        self.block3_conv2 = tf.keras.layers.Conv2D(256, (3, 3),\n",
    "                                                   name='block3_conv2',\n",
    "                                                   padding='same')\n",
    "        self.block3_bn = tf.keras.layers.BatchNormalization()\n",
    "        self.block3_act = tf.keras.layers.ReLU()\n",
    "        self.block3_pool = tf.keras.layers.MaxPooling2D((2, 2), strides=None,\n",
    "                                                        name='block3_pool')\n",
    "\n",
    "        self.block4_conv1 = tf.keras.layers.Conv2D(512, (3, 3),\n",
    "                                                   name='block4_conv1',\n",
    "                                                   activation='relu',\n",
    "                                                   padding='same')\n",
    "        self.block4_conv2 = tf.keras.layers.Conv2D(512, (3, 3),\n",
    "                                                   name='block4_conv2',\n",
    "                                                   padding='same')\n",
    "        self.block4_bn = tf.keras.layers.BatchNormalization()\n",
    "        self.block4_act = tf.keras.layers.ReLU()\n",
    "        self.block4_dropout = tf.keras.layers.Dropout(0.5)\n",
    "        self.block4_pool = tf.keras.layers.MaxPooling2D((2, 2), strides=None,\n",
    "                                                        name='block4_pool')\n",
    "\n",
    "        self.block5_conv1 = tf.keras.layers.Conv2D(1024, (3, 3),\n",
    "                                                   name='block5_conv1',\n",
    "                                                   activation='relu',\n",
    "                                                   padding='same')\n",
    "        self.block5_conv2 = tf.keras.layers.Conv2D(1024, (3, 3),\n",
    "                                                   name='block5_conv2',\n",
    "                                                   padding='same')\n",
    "        self.block5_bn = tf.keras.layers.BatchNormalization()\n",
    "        self.block5_act = tf.keras.layers.ReLU()\n",
    "        self.block5_dropout = tf.keras.layers.Dropout(0.5)\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        z1 = self.block1_conv1(x)\n",
    "        z1 = self.block1_conv2(z1)\n",
    "        z1 = self.block1_bn(z1)\n",
    "        z1 = self.block1_act(z1)\n",
    "        z1_pool = self.block1_pool(z1)\n",
    "\n",
    "        z2 = self.block2_conv1(z1_pool)\n",
    "        z2 = self.block2_conv2(z2)\n",
    "        z2 = self.block2_bn(z2)\n",
    "        z2 = self.block2_act(z2)\n",
    "        z2_pool = self.block2_pool(z2)\n",
    "\n",
    "        z3 = self.block3_conv1(z2_pool)\n",
    "        z3 = self.block3_conv2(z3)\n",
    "        z3 = self.block3_bn(z3)\n",
    "        z3 = self.block3_act(z3)\n",
    "        z3_pool = self.block3_pool(z3)\n",
    "\n",
    "        z4 = self.block4_conv1(z3_pool)\n",
    "        z4 = self.block4_conv2(z4)\n",
    "        z4 = self.block4_bn(z4)\n",
    "        z4 = self.block4_act(z4)\n",
    "        z4_dropout = self.block4_dropout(z4)\n",
    "        z4_pool = self.block4_pool(z4_dropout)\n",
    "\n",
    "        z5 = self.block5_conv1(z4_pool)\n",
    "        z5 = self.block5_conv2(z5)\n",
    "        z5 = self.block5_bn(z5)\n",
    "        z5 = self.block5_act(z5)\n",
    "        z5_dropout = self.block5_dropout(z5)\n",
    "\n",
    "        return z1, z2, z3, z4_dropout, z5_dropout\n",
    "\n",
    "\n",
    "class Decoder(Model):\n",
    "    def __init__(self, class_num):\n",
    "        super().__init__()\n",
    "        # Network\n",
    "        self.block6_up = tf.keras.layers.UpSampling2D(size=(2, 2))\n",
    "        self.block6_conv1 = tf.keras.layers.Conv2D(512, (2, 2),\n",
    "                                                   name='block6_conv1',\n",
    "                                                   activation='relu',\n",
    "                                                   padding='same')\n",
    "        self.block6_conv2 = tf.keras.layers.Conv2D(512, (3, 3),\n",
    "                                                   name='block6_conv2',\n",
    "                                                   activation='relu',\n",
    "                                                   padding='same')\n",
    "        self.block6_conv3 = tf.keras.layers.Conv2D(512, (3, 3),\n",
    "                                                   name='block6_conv3',\n",
    "                                                   padding='same')\n",
    "        self.block6_bn = tf.keras.layers.BatchNormalization()\n",
    "        self.block6_act = tf.keras.layers.ReLU()\n",
    "\n",
    "        self.block7_up = tf.keras.layers.UpSampling2D(size=(2, 2))\n",
    "        self.block7_conv1 = tf.keras.layers.Conv2D(256, (2, 2),\n",
    "                                                   name='block7_conv1',\n",
    "                                                   activation='relu',\n",
    "                                                   padding='same')\n",
    "        self.block7_conv2 = tf.keras.layers.Conv2D(256, (3, 3),\n",
    "                                                   name='block7_conv2',\n",
    "                                                   activation='relu',\n",
    "                                                   padding='same')\n",
    "        self.block7_conv3 = tf.keras.layers.Conv2D(256, (3, 3),\n",
    "                                                   name='block7_conv3',\n",
    "                                                   padding='same')\n",
    "        self.block7_bn = tf.keras.layers.BatchNormalization()\n",
    "        self.block7_act = tf.keras.layers.ReLU()\n",
    "\n",
    "        self.block8_up = tf.keras.layers.UpSampling2D(size=(2, 2))\n",
    "        self.block8_conv1 = tf.keras.layers.Conv2D(128, (2, 2),\n",
    "                                                   name='block8_conv1',\n",
    "                                                   activation='relu',\n",
    "                                                   padding='same')\n",
    "        self.block8_conv2 = tf.keras.layers.Conv2D(128, (3, 3),\n",
    "                                                   name='block8_conv2',\n",
    "                                                   activation='relu',\n",
    "                                                   padding='same')\n",
    "        self.block8_conv3 = tf.keras.layers.Conv2D(128, (3, 3),\n",
    "                                                   name='block8_conv3',\n",
    "                                                   padding='same')\n",
    "        self.block8_bn = tf.keras.layers.BatchNormalization()\n",
    "        self.block8_act = tf.keras.layers.ReLU()\n",
    "\n",
    "        self.block9_up = tf.keras.layers.UpSampling2D(size=(2, 2))\n",
    "        self.block9_conv1 = tf.keras.layers.Conv2D(64, (2, 2),\n",
    "                                                   name='block9_conv1',\n",
    "                                                   activation='relu',\n",
    "                                                   padding='same')\n",
    "        self.block9_conv2 = tf.keras.layers.Conv2D(64, (3, 3),\n",
    "                                                   name='block9_conv2',\n",
    "                                                   activation='relu',\n",
    "                                                   padding='same')\n",
    "        self.block9_conv3 = tf.keras.layers.Conv2D(64, (3, 3),\n",
    "                                                   name='block9_conv3',\n",
    "                                                   padding='same')\n",
    "        self.block9_bn = tf.keras.layers.BatchNormalization()\n",
    "        self.block9_act = tf.keras.layers.ReLU()\n",
    "        self.output_conv = tf.keras.layers.Conv2D(class_num,\n",
    "                                                  (1, 1), name='output_conv',\n",
    "                                                  activation='sigmoid')\n",
    "\n",
    "    def call(self, z1, z2, z3, z4_dropout, z5_dropout):\n",
    "        z6_up = self.block6_up(z5_dropout)\n",
    "        z6 = self.block6_conv1(z6_up)\n",
    "        z6 = tf.keras.layers.concatenate([z4_dropout, z6], axis=3)\n",
    "        z6 = self.block6_conv2(z6)\n",
    "        z6 = self.block6_conv3(z6)\n",
    "        z6 = self.block6_bn(z6)\n",
    "        z6 = self.block6_act(z6)\n",
    "\n",
    "        z7_up = self.block7_up(z6)\n",
    "        z7 = self.block7_conv1(z7_up)\n",
    "        z7 = tf.keras.layers.concatenate([z3, z7], axis=3)\n",
    "        z7 = self.block7_conv2(z7)\n",
    "        z7 = self.block7_conv3(z7)\n",
    "        z7 = self.block7_bn(z7)\n",
    "        z7 = self.block7_act(z7)\n",
    "\n",
    "        z8_up = self.block8_up(z7)\n",
    "        z8 = self.block8_conv1(z8_up)\n",
    "        z8 = tf.keras.layers.concatenate([z2, z8], axis=3)\n",
    "        z8 = self.block8_conv2(z8)\n",
    "        z8 = self.block8_conv3(z8)\n",
    "        z8 = self.block8_bn(z8)\n",
    "        z8 = self.block8_act(z8)\n",
    "\n",
    "        z9_up = self.block9_up(z8)\n",
    "        z9 = self.block9_conv1(z9_up)\n",
    "        z9 = tf.keras.layers.concatenate([z1, z9], axis=3)\n",
    "        z9 = self.block9_conv2(z9)\n",
    "        z9 = self.block9_conv3(z9)\n",
    "        z9 = self.block9_bn(z9)\n",
    "        z9 = self.block9_act(z9)\n",
    "        y = self.output_conv(z9)\n",
    "        y = Activation('softmax')(y)\n",
    "\n",
    "        return y\n"
   ],
   "metadata": {
    "id": "wDXHXTp-2cSJ"
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# PSP net\n",
    "def Pyramid_Pooling_Module(features, f=64, p1=2, p2=3, p3=6):\n",
    "    shape = features.shape\n",
    "    red = GlobalAveragePooling2D()(features)\n",
    "    red = Reshape((1, 1, shape[-1]))(red)\n",
    "    red = Conv2D(filters=f, kernel_size=(1, 1), padding='same', use_bias=False)(\n",
    "        red)\n",
    "    red = BatchNormalization()(red)\n",
    "    red = Activation('relu')(red)\n",
    "    red = UpSampling2D(size=shape[1], interpolation='bilinear')(red)\n",
    "\n",
    "    orange = AveragePooling2D(pool_size=(p1))(features)\n",
    "    orange = Conv2D(filters=f, kernel_size=(1, 1), padding='same',\n",
    "                    use_bias=False)(orange)\n",
    "    orange = BatchNormalization()(orange)\n",
    "    orange = Activation('relu')(orange)\n",
    "    orange = UpSampling2D(size=p1, interpolation='bilinear')(orange)\n",
    "\n",
    "    blue = AveragePooling2D(pool_size=(p2))(features)\n",
    "    blue = Conv2D(filters=f, kernel_size=(1, 1), padding='same',\n",
    "                  use_bias=False)(blue)\n",
    "    blue = BatchNormalization()(blue)\n",
    "    blue = Activation('relu')(blue)\n",
    "    blue = UpSampling2D(size=p2, interpolation='bilinear')(blue)\n",
    "\n",
    "    green = AveragePooling2D(pool_size=(p3))(features)\n",
    "    green = Conv2D(filters=f, kernel_size=(1, 1), padding='same',\n",
    "                   use_bias=False)(green)\n",
    "    green = BatchNormalization()(green)\n",
    "    green = Activation('relu')(green)\n",
    "    green = UpSampling2D(size=p3, interpolation='bilinear')(green)\n",
    "\n",
    "    return Concatenate()([features, red, orange, blue, green])\n",
    "\n",
    "\n",
    "def PSPNet(inputs, classes=100):\n",
    "    inputs = Input(inputs)\n",
    "\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False,\n",
    "                          input_tensor=inputs)\n",
    "    base_model_features = base_model.get_layer('conv3_block4_add').output\n",
    "\n",
    "    x = Pyramid_Pooling_Module(base_model_features, f=64, p1=2, p2=4, p3=8)\n",
    "    x = UpSampling2D(size=8, interpolation='bilinear')(x)\n",
    "\n",
    "    x = Conv2D(filters=64, kernel_size=3, padding='same', use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    \"\"\" Outputs \"\"\"\n",
    "    x = Conv2D(classes, (1, 1), name='output_layer')(x)\n",
    "\n",
    "    if classes == 1:\n",
    "        x = Activation('sigmoid')(x)\n",
    "    else:\n",
    "        x = Activation('softmax')(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=x)\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# deellabv3\n",
    "\n",
    "def ASPP(image_features):\n",
    "    shape = image_features.shape\n",
    "\n",
    "    y_pool = AveragePooling2D(pool_size=(shape[1], shape[2]))(image_features)\n",
    "    y_pool = Conv2D(filters=32, kernel_size=1, padding='same', use_bias=False)(y_pool)\n",
    "    y_pool = BatchNormalization(name=f'bn_1')(y_pool)\n",
    "    y_pool = Activation('relu', name=f'relu_1')(y_pool)\n",
    "    y_pool = UpSampling2D((shape[1], shape[2]), interpolation=\"bilinear\")(y_pool)\n",
    "\n",
    "    y_1 = Conv2D(filters=32, kernel_size=1, padding='same', use_bias=False)(image_features)\n",
    "    y_1 = BatchNormalization(name=f'bn_2')(y_1)\n",
    "    y_1 = Activation('relu', name=f'relu_2')(y_1)\n",
    "\n",
    "    y_6 = Conv2D(filters=32, kernel_size=3, padding='same', dilation_rate=2, use_bias=False)(\n",
    "        image_features)\n",
    "    y_6 = BatchNormalization(name=f'bn_3')(y_6)\n",
    "    y_6 = Activation('relu', name=f'relu_3')(y_6)\n",
    "\n",
    "    y_12 = Conv2D(filters=32, kernel_size=1, padding='same', dilation_rate=4, use_bias=False)(\n",
    "        image_features)\n",
    "    y_12 = BatchNormalization(name=f'bn_4')(y_12)\n",
    "    y_12 = Activation('relu', name=f'relu_4')(y_12)\n",
    "\n",
    "    y_18 = Conv2D(filters=32, kernel_size=3, padding='same', dilation_rate=6, use_bias=False)(\n",
    "        image_features)\n",
    "    y_18 = BatchNormalization(name=f'bn_5')(y_18)\n",
    "    y_18 = Activation('relu', name=f'relu_5')(y_18)\n",
    "\n",
    "    y_c = Concatenate()([y_pool, y_1, y_6, y_12, y_18])\n",
    "\n",
    "    y = Conv2D(filters=32, kernel_size=1, padding='same', use_bias=False)(y_c)\n",
    "    y = BatchNormalization(name=f'bn_6')(y)\n",
    "    y = Activation('relu', name=f'relu_6')(y)\n",
    "\n",
    "    return y\n",
    "\n",
    "\n",
    "def DeepLabV3Plus(inputs, classes=1):\n",
    "    inputs = Input(inputs)\n",
    "\n",
    "    ResNext101, preprocess_input = Classifiers.get('resnext101')\n",
    "    base_model = ResNext101(input_tensor=inputs, weights='imagenet', classes=1, include_top=False)\n",
    "    high_level_image_features = base_model.get_layer('stage4_unit1_relu1').output\n",
    "\n",
    "    x_a = ASPP(high_level_image_features)\n",
    "    x_a = UpSampling2D(size=4, interpolation='bilinear')(x_a)\n",
    "\n",
    "    low_level_image_features = base_model.get_layer('stage2_unit1_relu1').output\n",
    "\n",
    "    x_b = Conv2D(filters=32, kernel_size=1, padding='same', use_bias=False)(\n",
    "        low_level_image_features)\n",
    "    x_b = BatchNormalization(name=f'bn_7')(x_b)\n",
    "    x_b = Activation('relu', name=f'relu_7')(x_b)\n",
    "\n",
    "    x = Concatenate()([x_a, x_b])\n",
    "\n",
    "    x = Conv2D(filters=32, kernel_size=3, padding='same', use_bias=False)(x)\n",
    "    x = BatchNormalization(name=f'bn_8')(x)\n",
    "    x = Activation('relu', name=f'relu_8')(x)\n",
    "\n",
    "    x = Conv2D(filters=32, kernel_size=3, padding='same', use_bias=False)(x)\n",
    "    x = BatchNormalization(name=f'bn_9')(x)\n",
    "    x = Activation('relu', name=f'relu_9')(x)\n",
    "\n",
    "    x = UpSampling2D(size=4, interpolation='bilinear')(x)\n",
    "\n",
    "    \"\"\" Outputs \"\"\"\n",
    "    x = Conv2D(classes, (1, 1), name='output_layer')(x)\n",
    "\n",
    "    if classes == 1:\n",
    "        x = Activation('sigmoid')(x)\n",
    "    else:\n",
    "        x = Activation('softmax')(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=x)\n",
    "\n",
    "    return model\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "CLASS_NAME = ['Background',\n",
    "              'Holds',\n",
    "              'Mat',\n",
    "              'Human',\n",
    "              ]\n",
    "\n",
    "\n",
    "class CustomMeanIoU(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name='custom_mean_iou', num_classes=OUTPUT_CLASSES, c_name=None, dtype=None):\n",
    "        super(CustomMeanIoU, self).__init__(name=name, dtype=dtype)\n",
    "        self.num_classes = num_classes\n",
    "        self.true_positives = [\n",
    "            self.add_weight('true_positive_{}'.format(i), initializer='zeros')\n",
    "            for i in range(self.num_classes)]\n",
    "        self.false_positives = [\n",
    "            self.add_weight('false_positive_{}'.format(i), initializer='zeros')\n",
    "            for i in range(self.num_classes)]\n",
    "        self.false_negatives = [\n",
    "            self.add_weight('false_negative_{}'.format(i), initializer='zeros')\n",
    "            for i in range(self.num_classes)]\n",
    "\n",
    "        if c_name is not None:\n",
    "            self.c_name = c_name\n",
    "        else:\n",
    "            self.c_name = [str(i) for i in range(self.num_classes)]\n",
    "\n",
    "        self.iot_dict = {}\n",
    "        for i in range(self.num_classes):\n",
    "            self.iot_dict[self.c_name[i]] = 0\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        \"\"\"\n",
    "        バッチごとにIoUを計算\n",
    "        :param y_true: \n",
    "        :param y_pred: \n",
    "        :param sample_weight: \n",
    "        :return: \n",
    "        \"\"\"\n",
    "\n",
    "        # スパース形式からone-hot形式に変換\n",
    "        y_true = tf.one_hot(tf.cast(y_true, tf.int32), self.num_classes)\n",
    "\n",
    "        # 不要な行を削除\n",
    "        y_true = tf.cast(tf.squeeze(y_true, axis=3), tf.int32)\n",
    "\n",
    "        # 最大値を持つインデックスを取得\n",
    "        y_pred = tf.argmax(y_pred, axis=-1)\n",
    "        y_pred = tf.one_hot(tf.cast(y_pred, tf.int32), self.num_classes)\n",
    "\n",
    "        # 各クラスごとにTrue Positive, False Positive, False Negativeを計算\n",
    "        for i in range(self.num_classes):\n",
    "            y_true_i = tf.cast(tf.equal(y_true, i), tf.int32)\n",
    "            y_pred_i = tf.cast(tf.equal(y_pred, i), tf.int32)\n",
    "\n",
    "            # int32型のテンソルをfloat32型にキャスト\n",
    "            true_positives_update = tf.cast(tf.reduce_sum(y_true_i * y_pred_i),\n",
    "                                            tf.float32)\n",
    "            false_positives_update = tf.cast(\n",
    "                tf.reduce_sum((1 - y_true_i) * y_pred_i), tf.float32)\n",
    "            false_negatives_update = tf.cast(\n",
    "                tf.reduce_sum(y_true_i * (1 - y_pred_i)), tf.float32)\n",
    "\n",
    "            self.true_positives[i].assign_add(true_positives_update)\n",
    "            self.false_positives[i].assign_add(false_positives_update)\n",
    "            self.false_negatives[i].assign_add(false_negatives_update)\n",
    "\n",
    "    def result(self):\n",
    "        \"\"\"\n",
    "        各クラスごとにIoUを計算\n",
    "        :return: \n",
    "        \"\"\"\n",
    "        # 各クラスごとにIoUを計算\n",
    "        for i in range(self.num_classes):\n",
    "            iou = self.true_positives[i] / (\n",
    "                    self.true_positives[i] + self.false_positives[i] +\n",
    "                    self.false_negatives[i] + tf.keras.backend.epsilon())\n",
    "            self.iot_dict[self.c_name[i]] = iou\n",
    "        return self.iot_dict\n",
    "\n",
    "    def reset_state(self):\n",
    "        \"\"\"\n",
    "        エポック終わりにすべての結果をリセットする\n",
    "        :return: \n",
    "        \"\"\"\n",
    "        for i in range(self.num_classes):\n",
    "            self.true_positives[i].assign(0)\n",
    "            self.false_positives[i].assign(0)\n",
    "            self.false_negatives[i].assign(0)\n",
    "\n",
    "        # iot_dict を初期化\n",
    "        for i in range(self.num_classes):\n",
    "            self.iot_dict[self.c_name[i]] = 0\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(CustomMeanIoU, self).get_config()\n",
    "        config.update({\n",
    "            'num_classes': self.num_classes,\n",
    "            'c_name': self.c_name\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-09T02:10:27.546125Z",
     "iopub.status.busy": "2022-08-09T02:10:27.545680Z",
     "iopub.status.idle": "2022-08-09T02:10:28.115166Z",
     "shell.execute_reply": "2022-08-09T02:10:28.114394Z"
    },
    "id": "6he36HK5uKAc"
   },
   "outputs": [],
   "source": [
    "# model = unet_model(output_channels=OUTPUT_CLASSES)\n",
    "\n",
    "# PSP net\n",
    "# model = PSPNet((IMAGE_WIDTH, IMAGE_HEIGHT, 3), classes=OUTPUT_CLASSES)\n",
    "\n",
    "# U-net\n",
    "# model = UNet(OUTPUT_CLASSES)\n",
    "\n",
    "# deeplab\n",
    "model = DeepLabV3Plus((IMAGE_WIDTH, IMAGE_HEIGHT, 3), classes=OUTPUT_CLASSES)\n",
    "\n",
    "iou = CustomMeanIoU(name=\"iou\", c_name=CLASS_NAME)\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=0.01),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "                  from_logits=False),\n",
    "              metrics=['accuracy', iou]\n",
    "              )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "トレーニングする前に、モデルが何を予測するかを試してみましょう。"
   ],
   "metadata": {
    "collapsed": false,
    "id": "NXdxLbyo2cSJ"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-09T02:10:28.193567Z",
     "iopub.status.busy": "2022-08-09T02:10:28.193142Z",
     "iopub.status.idle": "2022-08-09T02:10:28.197818Z",
     "shell.execute_reply": "2022-08-09T02:10:28.197160Z"
    },
    "id": "UwvIKLZPtxV_"
   },
   "outputs": [],
   "source": [
    "def create_mask(pred_mask):\n",
    "    pred_mask = tf.argmax(pred_mask, axis=-1)\n",
    "    pred_mask = pred_mask[..., tf.newaxis]\n",
    "    return pred_mask[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-09T02:10:28.201137Z",
     "iopub.status.busy": "2022-08-09T02:10:28.200526Z",
     "iopub.status.idle": "2022-08-09T02:10:28.205328Z",
     "shell.execute_reply": "2022-08-09T02:10:28.204732Z"
    },
    "id": "YLNsrynNtx4d"
   },
   "outputs": [],
   "source": [
    "def show_predictions(batch=val_batches, num=1, predict_model=model):\n",
    "    count = 0\n",
    "    for images, masks in batch.take(1):\n",
    "        for image, mask in zip(images, masks):\n",
    "            sample_image, sample_mask = image, mask\n",
    "            pred_input = tf.expand_dims(sample_image, axis=0)\n",
    "            pred_mask = predict_model.predict(pred_input)\n",
    "\n",
    "            display([sample_image, sample_mask, create_mask(pred_mask)])\n",
    "\n",
    "            count += 1\n",
    "            if count == num:\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-09T02:10:28.208703Z",
     "iopub.status.busy": "2022-08-09T02:10:28.208208Z",
     "iopub.status.idle": "2022-08-09T02:10:30.564302Z",
     "shell.execute_reply": "2022-08-09T02:10:30.563648Z"
    },
    "id": "X_1CC0T4dho3"
   },
   "outputs": [],
   "source": [
    "show_predictions(num=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "22AyVYWQdkgk"
   },
   "source": [
    "以下に定義されるコールバックは、トレーニング中にモデルがどのように改善するかを観測するために使用されます。"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class CustomModelCheckpoint(tf.keras.callbacks.ModelCheckpoint):\n",
    "    \"\"\"\n",
    "    Custom Metricsが特定の閾値を超えたときだけモデルを保存するチェックポイント\n",
    "    thresholdsはkeyがmetrics名、値が閾値となるDictionary\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, filepath, thresholds, mode='auto', inverse=False):\n",
    "        super(CustomModelCheckpoint, self).__init__(filepath)\n",
    "\n",
    "        self.filepath = filepath\n",
    "        self.thresholds = thresholds\n",
    "        self.inverse = inverse\n",
    "        self.max_log = {}\n",
    "\n",
    "        for k, v in self.thresholds.items():\n",
    "            self.max_log[k] = 0\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        \"\"\"\n",
    "        logsにmetricsに指定した内容が（validationの方はval_というプレフィックス付きで）入っている。\n",
    "        \"\"\"\n",
    "        logs = logs or {}\n",
    "        filepath = self.filepath.format(epoch=epoch, **logs)\n",
    "\n",
    "        save = True\n",
    "        for k, v in self.thresholds.items():\n",
    "            if self.inverse:\n",
    "                if (k in logs and logs[k] > v) or np.isnan(logs[k]):\n",
    "                    save = False\n",
    "                    break\n",
    "            else:\n",
    "                if (k in logs and logs[k] < v) or (\n",
    "                        logs[k] <= self.max_log[k]) or np.isnan(logs[k]):\n",
    "                    print(\"not saved {} : {}\".format(k, logs[k]))\n",
    "                    save = False\n",
    "                    break\n",
    "\n",
    "        if save:\n",
    "            print(\"Save Model\")\n",
    "            self.model.save(filepath, overwrite=True)\n",
    "\n",
    "            # 新しいモデルが保存された時の結果を保存しておく\n",
    "            for k, v in self.thresholds.items():\n",
    "                self.max_log[k] = logs[k]\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        show_predictions()\n",
    "        print('\\nSample Prediction after epoch {}\\n'.format(epoch + 1))\n",
    "\n",
    "        for k in self.params.keys():\n",
    "            print(f\"{k}: {self.params[k]}\")\n",
    "\n",
    "\n",
    "# 実行しているディレクトリないに checkpoint ディレクトリを作成\n",
    "checkpoint_filepath = './checkpoint'"
   ],
   "metadata": {
    "id": "cfVHtcV02cSK"
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "VAL_SUBSPLITS = 1\n",
    "VALIDATION_STEPS = max(len(val_images) // BATCH_SIZE // VAL_SUBSPLITS, 1)\n",
    "print(f\"VALIDATION_STEPS: {VALIDATION_STEPS}\")\n",
    "print(f\"BATCH_SIZE: {BATCH_SIZE}\")"
   ],
   "metadata": {
    "id": "p7PS54kf2cSK",
    "outputId": "a65f4e8b-acb6-4408-90b6-6ddfc9a38aa5",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-09T02:10:30.578866Z",
     "iopub.status.busy": "2022-08-09T02:10:30.578360Z",
     "iopub.status.idle": "2022-08-09T02:12:30.388035Z",
     "shell.execute_reply": "2022-08-09T02:12:30.387324Z"
    },
    "id": "StKDH_B9t4SD",
    "outputId": "37662082-8051-4a8f-bc31-4c094b562b38",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    }
   },
   "outputs": [],
   "source": [
    "model_history = model.fit(train_batches,\n",
    "                          epochs=EPOCHS,\n",
    "                          steps_per_epoch=STEPS_PER_EPOCH,\n",
    "                          validation_steps=VALIDATION_STEPS,\n",
    "                          validation_data=val_batches,\n",
    "                          callbacks=[CustomModelCheckpoint(\n",
    "                              checkpoint_filepath,\n",
    "                              thresholds={\n",
    "                                  'accuracy': 0.9,\n",
    "                                  'val_accuracy': 0.75,\n",
    "                                  'Holds': 0.6,\n",
    "                                  'val_Holds': 0.5\n",
    "                              }\n",
    "                          )\n",
    "                          ]\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "for key in model_history.history.keys():\n",
    "    print(f\"key: {key}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# CLASS_NAME に含まれる値をプロットする\n",
    "plt.figure()\n",
    "\n",
    "for key in model_history.history.keys():\n",
    "    if key in CLASS_NAME:\n",
    "        # プロットの色や形を変更する\n",
    "        plt.plot(model_history.history[key], 'r', label=key)\n",
    "        plt.plot(model_history.history['val_' + key], 'bo', label='val_' + key)\n",
    "        plt.title(key)\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel(key)\n",
    "        plt.ylim([0, 1])\n",
    "        plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-09T02:12:30.392294Z",
     "iopub.status.busy": "2022-08-09T02:12:30.391730Z",
     "iopub.status.idle": "2022-08-09T02:12:30.506611Z",
     "shell.execute_reply": "2022-08-09T02:12:30.505732Z"
    },
    "id": "P_mu0SAbt40Q",
    "outputId": "d469115b-9b0f-4631-f3dc-f60a76511346",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    }
   },
   "outputs": [],
   "source": [
    "loss = model_history.history['loss']\n",
    "val_loss = model_history.history['val_loss']\n",
    "\n",
    "x_window = min(EPOCHS - 10, len(loss))\n",
    "start = EPOCHS - x_window\n",
    "\n",
    "x_size = min(x_window, len(loss))\n",
    "\n",
    "loss = loss[-x_size:]\n",
    "val_loss = val_loss[-x_size:]\n",
    "x = range(start, start + x_size)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x, loss, 'r', label='Training loss')\n",
    "plt.plot(x, val_loss, 'bo', label='Validation loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss Value')\n",
    "# plt.ylim([0, 1])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "acc = model_history.history['accuracy']\n",
    "val_acc = model_history.history['val_accuracy']\n",
    "\n",
    "x_window = min(EPOCHS - 10, len(acc))\n",
    "start = EPOCHS - x_window\n",
    "\n",
    "x_size = min(x_window, len(acc))\n",
    "\n",
    "acc = acc[-x_size:]\n",
    "val_acc = val_acc[-x_size:]\n",
    "x = range(start, start + x_size)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x, acc, 'r', label='Training accuracy')\n",
    "plt.plot(x, val_acc, 'bo', label='Validation accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy Value')\n",
    "# plt.ylim([0.6, 1])\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "E-TN3FC32cSK",
    "outputId": "435e09fa-41a2-400b-e60f-f90409535d7e",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    }
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "unP3cnxo_N72"
   },
   "source": [
    "## 予測する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7BVXldSo-0mW"
   },
   "source": [
    "いくつか予測を行ってみましょう。時間の節約重視の場合はエポック数を少なくしますが、高精度の結果重視の場合はエポック数を増やして設定します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "checkpoint_filepath = \"./checkpoint\"\n",
    "\n",
    "# カスタムオブジェクトの辞書を作成\n",
    "custom_objects = {\"CustomMeanIoU\": CustomMeanIoU(name=\"iou\", c_name=CLASS_NAME)}\n",
    "\n",
    "# 精度が良かったモデルを読み込む\n",
    "checkpoint_model = tf.keras.models.load_model(checkpoint_filepath, custom_objects=custom_objects)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "show_predictions(train_batches, 2, checkpoint_model)"
   ],
   "metadata": {
    "id": "oDUMkK8l2cSK",
    "outputId": "973e3075-4f45-4ff0-e351-bb9667e30ab9",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 784
    }
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-09T02:12:30.510184Z",
     "iopub.status.busy": "2022-08-09T02:12:30.509706Z",
     "iopub.status.idle": "2022-08-09T02:12:31.965346Z",
     "shell.execute_reply": "2022-08-09T02:12:31.964682Z"
    },
    "id": "ikrzoG24qwf5",
    "outputId": "0c8799ee-840b-450f-8e78-9ce3708fac1e",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    }
   },
   "outputs": [],
   "source": [
    "show_predictions(val_batches, 4, checkpoint_model)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# test データの結果を出力\n",
    "show_predictions(test_batches, 2, checkpoint_model)"
   ],
   "metadata": {
    "id": "BmJwjD1t2cSK",
    "outputId": "5e45994e-27c7-425b-82a8-540428b7f138",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 837
    }
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# test データを使用して評価\n",
    "result = checkpoint_model.evaluate(test_batches)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QAwvlgSNoK3o"
   },
   "source": [
    " ## オプション: 不均衡なクラスとクラスの重み"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gbwo3DZ-9TxM"
   },
   "source": [
    "つまり、このチュートリアルのサンプル重みを作るには、`(data, label)` ペアを取って `(data, label, sample_weight)` トリプルを返す関数が必要となります。`sample_weight` は各ピクセルのクラス重みを含む 1-channel の画像です。\n",
    "\n",
    "実装を可能な限り単純にするために、ラベルを`class_weight` リストのインデックスとして使用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-09T02:12:31.996236Z",
     "iopub.status.busy": "2022-08-09T02:12:31.995742Z",
     "iopub.status.idle": "2022-08-09T02:12:31.999775Z",
     "shell.execute_reply": "2022-08-09T02:12:31.999240Z"
    },
    "id": "DlG-n2Ugo8Jc"
   },
   "outputs": [],
   "source": [
    "def add_sample_weights(image, label):\n",
    "    # The weights for each class, with the constraint that:\n",
    "    #     sum(class_weights) == 1.0\n",
    "    class_weights = tf.constant([0.5, 2.0, 2.0, 1.5])\n",
    "    class_weights = class_weights / tf.reduce_sum(class_weights)\n",
    "\n",
    "    # Create an image of `sample_weights` by using the label at each pixel as an\n",
    "    # index into the `class weights` .\n",
    "    sample_weights = tf.gather(class_weights, indices=tf.cast(label, tf.int32))\n",
    "\n",
    "    return image, label, sample_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hLH_NvH2UrXU"
   },
   "source": [
    "この結果、データセットの各要素には、3 つの画像が含まれます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-09T02:12:32.002729Z",
     "iopub.status.busy": "2022-08-09T02:12:32.002295Z",
     "iopub.status.idle": "2022-08-09T02:12:32.053546Z",
     "shell.execute_reply": "2022-08-09T02:12:32.052912Z"
    },
    "id": "SE_ezRSFRCnE"
   },
   "outputs": [],
   "source": [
    "train_batches.map(add_sample_weights).element_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yc-EpIzaRbSL"
   },
   "source": [
    "これで、この重み付けが付けられたデータセットでモデルをトレーニングできるようになりました。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-09T02:12:32.056878Z",
     "iopub.status.busy": "2022-08-09T02:12:32.056223Z",
     "iopub.status.idle": "2022-08-09T02:12:32.485808Z",
     "shell.execute_reply": "2022-08-09T02:12:32.484918Z"
    },
    "id": "QDWipedAoOQe"
   },
   "outputs": [],
   "source": [
    "weighted_model = DeepLabV3Plus(OUTPUT_CLASSES)\n",
    "weighted_model.compile(optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=0.01),\n",
    "                       loss=tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "                           from_logits=False),\n",
    "                       metrics=['accuracy', iou]\n",
    "                       )\n",
    "weighted_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-09T02:12:32.490054Z",
     "iopub.status.busy": "2022-08-09T02:12:32.489404Z",
     "iopub.status.idle": "2022-08-09T02:12:36.359074Z",
     "shell.execute_reply": "2022-08-09T02:12:36.358379Z"
    },
    "id": "btEFKc1xodGR"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "checkpoint_filepath = './checkpoint_weighted'\n",
    "weighted_model_history = model.fit(train_batches,\n",
    "                                   epochs=EPOCHS,\n",
    "                                   steps_per_epoch=STEPS_PER_EPOCH,\n",
    "                                   validation_steps=VALIDATION_STEPS,\n",
    "                                   validation_data=val_batches,\n",
    "                                   callbacks=[CustomModelCheckpoint(\n",
    "                                       checkpoint_filepath,\n",
    "                                       thresholds={\n",
    "                                           'accuracy': 0.8,\n",
    "                                           'val_accuracy': 0.7,\n",
    "                                           'Holds': 0.6,\n",
    "                                           'val_Holds': 0.5\n",
    "                                       }\n",
    "                                   )\n",
    "                                   ]\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "loss = weighted_model_history.history['loss']\n",
    "val_loss = weighted_model_history.history['val_loss']\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(weighted_model_history.epoch, loss, 'r', label='Training loss')\n",
    "plt.plot(range(len(val_loss)), val_loss, 'bo', label='Validation loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss Value')\n",
    "# plt.ylim([0, 1])\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "BPaG2iLi2cSL"
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "loss = weighted_model_history.history['loss']\n",
    "val_loss = weighted_model_history.history['val_loss']\n",
    "\n",
    "x = range(len(loss))\n",
    "plt.figure()\n",
    "plt.plot(x, loss, 'r', label='Training loss')\n",
    "plt.plot(x, val_loss, 'bo', label='Validation loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss Value')\n",
    "# plt.ylim([0, 1])\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "bduRwqve2cSL"
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "show_predictions(test_batches, 3, weighted_model)\n"
   ],
   "metadata": {
    "id": "KQ-M7J8o2cSL"
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R24tahEqmSCk"
   },
   "source": [
    "\n",
    "## 次のステップ\n",
    "\n",
    "これで画像セグメンテーションとは何か、それがどのように機能するかについての知識が得られたはずです。このチュートリアルは、異なる中間レイヤー出力や、異なる事前トレーニング済みモデルでも試すことができます。また、Kaggle がホストしている [Carvana](https://www.kaggle.com/c/carvana-image-masking-challenge/overview) 画像マスキングチャレンジに挑戦してみることもお勧めです。\n",
    "\n",
    "[Tensorflow Object Detection API](https://github.com/tensorflow/models/blob/master/research/object_detection/README.md) を参照して、独自のデータで再トレーニング可能な別のモデルを確認するのも良いでしょう。トレーニング済みのモデルは、[TensorFlow Hub](https://www.tensorflow.org/hub/tutorials/tf2_object_detection#optional) にあります。"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "id": "IUW-ApRc2cSL"
   },
   "execution_count": null
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
