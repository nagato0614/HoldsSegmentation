{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cZCM65CBt1CJ"
   },
   "source": [
    "##### Copyright 2019 The TensorFlow Authors.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2022-08-09T02:10:11.391957Z",
     "iopub.status.busy": "2022-08-09T02:10:11.391697Z",
     "iopub.status.idle": "2022-08-09T02:10:11.396056Z",
     "shell.execute_reply": "2022-08-09T02:10:11.395458Z"
    },
    "id": "JOgMcEajtkmg"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rCSP-dbMw88x"
   },
   "source": [
    "# 画像セグメンテーション"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NEWs8JXRuGex"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td><a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/images/segmentation\">     <img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">     TensorFlow.org で表示</a></td>\n",
    "  <td><a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/ja/tutorials/images/segmentation.ipynb\">     <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">     Google Colab で実行</a></td>\n",
    "  <td><a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/ja/tutorials/images/segmentation.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">GitHub でソースを表示</a></td>\n",
    "  <td><a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/ja/tutorials/images/segmentation.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">ノートブックをダウンロード</a></td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sMP7mglMuGT2"
   },
   "source": [
    "このチュートリアルでは、修正した <a href=\"https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/\" class=\"external\">U-Net</a> を使用した画像セグメンテーションのタスクに焦点を当てます。\n",
    "\n",
    "## 画像セグメンテーションとは\n",
    "\n",
    "画像分類タスクでは、ネットワークが各入力画像にラベル（またはクラス）を割り当てますが、そのオブジェクトの形状やどのピクセルがどのオブジェクトに属しているかなどを知りたい場合はどうすればよいでしょうか。この場合、画像のピクセルごとにクラスを割り当てようと考えるでしょう。このタスクはセグメンテーションとして知られています。セグメンテーションモデルは、画像に関してはるかに詳細な情報を返します。画像セグメンテーションには、医用イメージング、自動走行車、衛星撮像など、数多くの用途があります。\n",
    "\n",
    "このチュートリアルでは [Oxford-IIIT Pet Dataset](https://www.robots.ox.ac.uk/~vgg/data/pets/)（Parkhi <em>et al</em>）を使用します。データセットには、37 種のペット品種と、品種当たり 200 枚の画像（train と test split で約 100 枚ずつ）が含まれます。それぞれの画像には対応するラベルとピクセル方向のマスクが含まれます。マスクは各ピクセルのクラスラベルです。各ピクセルには、次のいずれかのカテゴリが指定されます。\n",
    "\n",
    "- クラス 1 : ペットに属するピクセル。\n",
    "- クラス 2 : ペットと境界のピクセル。\n",
    "- クラス 3: 上記のいずれにも該当しない、または周囲のピクセル。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-09T02:10:18.800474Z",
     "iopub.status.busy": "2022-08-09T02:10:18.799894Z",
     "iopub.status.idle": "2022-08-09T02:10:19.270491Z",
     "shell.execute_reply": "2022-08-09T02:10:19.269742Z"
    },
    "id": "g87--n2AtyO_"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow_examples.models.pix2pix import pix2pix\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oWe0_rQM4JbC"
   },
   "source": [
    "## Oxford-IIIT ペットデータセットをダウンロードする\n",
    "\n",
    "データセットは [TensorFlow Datasets から入手できます](https://www.tensorflow.org/datasets/catalog/oxford_iiit_pet)。セグメンテーションマスクはバージョン 3 以上に含まれています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-09T02:10:19.274991Z",
     "iopub.status.busy": "2022-08-09T02:10:19.274336Z",
     "iopub.status.idle": "2022-08-09T02:10:23.571781Z",
     "shell.execute_reply": "2022-08-09T02:10:23.571014Z"
    },
    "id": "40ITeStwDwZb"
   },
   "outputs": [],
   "source": [
    "dataset, info = tfds.load('oxford_iiit_pet:3.*.*', with_info=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rJcVdj_U4vzf"
   },
   "source": [
    "また、画像の色値は `[0,1]` の範囲に正規化されています。最後に、上記で説明したとおり、セグメンテーションのマスクは {1, 2, 3} のいずれかでラベル付けされています。便宜上、セグメンテーションマスクから 1 を減算して、ラベルを {0, 1, 2} としましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-09T02:10:23.576072Z",
     "iopub.status.busy": "2022-08-09T02:10:23.575491Z",
     "iopub.status.idle": "2022-08-09T02:10:23.579302Z",
     "shell.execute_reply": "2022-08-09T02:10:23.578731Z"
    },
    "id": "FD60EbcAQqov"
   },
   "outputs": [],
   "source": [
    "def normalize(input_image, input_mask):\n",
    "  input_image = tf.cast(input_image, tf.float32) / 255.0\n",
    "  input_mask -= 1\n",
    "  return input_image, input_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-09T02:10:23.582497Z",
     "iopub.status.busy": "2022-08-09T02:10:23.582055Z",
     "iopub.status.idle": "2022-08-09T02:10:23.585901Z",
     "shell.execute_reply": "2022-08-09T02:10:23.585297Z"
    },
    "id": "Zf0S67hJRp3D"
   },
   "outputs": [],
   "source": [
    "def load_image(datapoint):\n",
    "  input_image = tf.image.resize(datapoint['image'], (128, 128))\n",
    "  input_mask = tf.image.resize(datapoint['segmentation_mask'], (128, 128))\n",
    "\n",
    "  input_image, input_mask = normalize(input_image, input_mask)\n",
    "\n",
    "  return input_image, input_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "65-qHTjX5VZh"
   },
   "source": [
    "データセットにはすでに必要となる training と test split が含まれているため、そのまま同じ split を使用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-09T02:10:23.589137Z",
     "iopub.status.busy": "2022-08-09T02:10:23.588533Z",
     "iopub.status.idle": "2022-08-09T02:10:23.591881Z",
     "shell.execute_reply": "2022-08-09T02:10:23.591301Z"
    },
    "id": "yHwj2-8SaQli"
   },
   "outputs": [],
   "source": [
    "TRAIN_LENGTH = info.splits['train'].num_examples\n",
    "BATCH_SIZE = 10\n",
    "BUFFER_SIZE = 10\n",
    "STEPS_PER_EPOCH = TRAIN_LENGTH // BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-09T02:10:23.595063Z",
     "iopub.status.busy": "2022-08-09T02:10:23.594551Z",
     "iopub.status.idle": "2022-08-09T02:10:23.691655Z",
     "shell.execute_reply": "2022-08-09T02:10:23.690951Z"
    },
    "id": "39fYScNz9lmo"
   },
   "outputs": [],
   "source": [
    "train_images = dataset['train'].map(load_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "test_images = dataset['test'].map(load_image, num_parallel_calls=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T9hGHyg8L3Y1"
   },
   "source": [
    "次のクラスは、画像をランダムにフリップする単純な拡張を実行します。詳細は、[画像のデータ拡張](data_augmentation.ipynb)チュートリアルをご覧ください。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-09T02:10:23.696326Z",
     "iopub.status.busy": "2022-08-09T02:10:23.695643Z",
     "iopub.status.idle": "2022-08-09T02:10:23.700499Z",
     "shell.execute_reply": "2022-08-09T02:10:23.699894Z"
    },
    "id": "fUWdDJRTL0PP"
   },
   "outputs": [],
   "source": [
    "class Augment(tf.keras.layers.Layer):\n",
    "  def __init__(self, seed=42):\n",
    "    super().__init__()\n",
    "    # both use the same seed, so they'll make the same random changes.\n",
    "    self.augment_inputs = tf.keras.layers.RandomFlip(mode=\"horizontal\", seed=seed)\n",
    "    self.augment_labels = tf.keras.layers.RandomFlip(mode=\"horizontal\", seed=seed)\n",
    "  \n",
    "  def call(self, inputs, labels):\n",
    "    inputs = self.augment_inputs(inputs)\n",
    "    labels = self.augment_labels(labels)\n",
    "    return inputs, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xTIbNIBdcgL3"
   },
   "source": [
    "入力パイプラインをビルドし、入力をバッチ処理した後に拡張を適用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-09T02:10:23.703658Z",
     "iopub.status.busy": "2022-08-09T02:10:23.703414Z",
     "iopub.status.idle": "2022-08-09T02:10:23.891329Z",
     "shell.execute_reply": "2022-08-09T02:10:23.890577Z"
    },
    "id": "VPscskQcNCx4"
   },
   "outputs": [],
   "source": [
    "train_batches = (\n",
    "    train_images\n",
    "    .cache()\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .repeat()\n",
    "    .map(Augment())\n",
    "    .prefetch(buffer_size=tf.data.AUTOTUNE))\n",
    "\n",
    "test_batches = test_images.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xa3gMAE_9qNa"
   },
   "source": [
    "画像サンプルと対応するデータセットのマスクを可視化しましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-09T02:10:23.896005Z",
     "iopub.status.busy": "2022-08-09T02:10:23.895096Z",
     "iopub.status.idle": "2022-08-09T02:10:23.900331Z",
     "shell.execute_reply": "2022-08-09T02:10:23.899689Z"
    },
    "id": "3N2RPAAW9q4W"
   },
   "outputs": [],
   "source": [
    "def display(display_list):\n",
    "  plt.figure(figsize=(15, 15))\n",
    "\n",
    "  title = ['Input Image', 'True Mask', 'Predicted Mask']\n",
    "\n",
    "  for i in range(len(display_list)):\n",
    "    plt.subplot(1, len(display_list), i+1)\n",
    "    plt.title(title[i])\n",
    "    plt.imshow(tf.keras.utils.array_to_img(display_list[i]))\n",
    "    plt.axis('off')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-09T02:10:23.903998Z",
     "iopub.status.busy": "2022-08-09T02:10:23.903432Z",
     "iopub.status.idle": "2022-08-09T02:10:25.897437Z",
     "shell.execute_reply": "2022-08-09T02:10:25.896611Z"
    },
    "id": "a6u_Rblkteqb"
   },
   "outputs": [],
   "source": [
    "for images, masks in train_batches.take(1):\n",
    "  sample_image, sample_mask = images[0], masks[0]\n",
    "  display([sample_image, sample_mask])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FAOe93FRMk3w"
   },
   "source": [
    "## モデルを定義する\n",
    "\n",
    "ここで使用されるモデルは修正された [U-Net](https://arxiv.org/abs/1505.04597) です。U-Net には、エンコーダ（ダウンサンプラー）とデコーダ（アップサンプラー）が含まれます。強力な特徴量を理解してトレーニング可能なパラメータ数を減らすため、MobileNetV2 というトレーニング済みモデルをエンコーダとして使用します。デコーダについてはアップサンプルブロックを使用しますが、これは TensorFlow Examples リポジトリの [pix2pix](https://github.com/tensorflow/examples/blob/master/tensorflow_examples/models/pix2pix/pix2pix.py) の例に実装済みです。（ノートブックの [pix2pix: 条件付き GAN による画像から画像への変換](../generative/pix2pix.ipynb)チュートリアルをご覧ください。）\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import Model\n",
    "\n",
    "\n",
    "class UNet(Model):\n",
    "    def __init__(self, class_num):\n",
    "        super().__init__()\n",
    "        # Network\n",
    "        self.enc = Encoder(class_num)\n",
    "        self.dec = Decoder(class_num)\n",
    "\n",
    "\n",
    "    def call(self, x, **kwargs):\n",
    "        z1, z2, z3, z4_dropout, z5_dropout = self.enc(x)\n",
    "        y = self.dec(z1, z2, z3, z4_dropout, z5_dropout)\n",
    "\n",
    "        return y\n",
    "\n",
    "\n",
    "class Encoder(Model):\n",
    "    def __init__(self, class_num):\n",
    "        super().__init__()\n",
    "        # Network\n",
    "        self.block1_conv1 = tf.keras.layers.Conv2D(64, (3, 3),\n",
    "                                                   name='block1_conv1',\n",
    "                                                   activation='relu',\n",
    "                                                   padding='same')\n",
    "        self.block1_conv2 = tf.keras.layers.Conv2D(64, (3, 3),\n",
    "                                                   name='block1_conv2',\n",
    "                                                   padding='same')\n",
    "        self.block1_bn = tf.keras.layers.BatchNormalization()\n",
    "        self.block1_act = tf.keras.layers.ReLU()\n",
    "        self.block1_pool = tf.keras.layers.MaxPooling2D((2, 2), strides=None,\n",
    "                                                        name='block1_pool')\n",
    "\n",
    "        self.block2_conv1 = tf.keras.layers.Conv2D(128, (3, 3),\n",
    "                                                   name='block2_conv1',\n",
    "                                                   activation='relu',\n",
    "                                                   padding='same')\n",
    "        self.block2_conv2 = tf.keras.layers.Conv2D(128, (3, 3),\n",
    "                                                   name='block2_conv2',\n",
    "                                                   padding='same')\n",
    "        self.block2_bn = tf.keras.layers.BatchNormalization()\n",
    "        self.block2_act = tf.keras.layers.ReLU()\n",
    "        self.block2_pool = tf.keras.layers.MaxPooling2D((2, 2), strides=None,\n",
    "                                                        name='block2_pool')\n",
    "\n",
    "        self.block3_conv1 = tf.keras.layers.Conv2D(256, (3, 3),\n",
    "                                                   name='block3_conv1',\n",
    "                                                   activation='relu',\n",
    "                                                   padding='same')\n",
    "        self.block3_conv2 = tf.keras.layers.Conv2D(256, (3, 3),\n",
    "                                                   name='block3_conv2',\n",
    "                                                   padding='same')\n",
    "        self.block3_bn = tf.keras.layers.BatchNormalization()\n",
    "        self.block3_act = tf.keras.layers.ReLU()\n",
    "        self.block3_pool = tf.keras.layers.MaxPooling2D((2, 2), strides=None,\n",
    "                                                        name='block3_pool')\n",
    "\n",
    "        self.block4_conv1 = tf.keras.layers.Conv2D(512, (3, 3),\n",
    "                                                   name='block4_conv1',\n",
    "                                                   activation='relu',\n",
    "                                                   padding='same')\n",
    "        self.block4_conv2 = tf.keras.layers.Conv2D(512, (3, 3),\n",
    "                                                   name='block4_conv2',\n",
    "                                                   padding='same')\n",
    "        self.block4_bn = tf.keras.layers.BatchNormalization()\n",
    "        self.block4_act = tf.keras.layers.ReLU()\n",
    "        self.block4_dropout = tf.keras.layers.Dropout(0.5)\n",
    "        self.block4_pool = tf.keras.layers.MaxPooling2D((2, 2), strides=None,\n",
    "                                                        name='block4_pool')\n",
    "\n",
    "        self.block5_conv1 = tf.keras.layers.Conv2D(1024, (3, 3),\n",
    "                                                   name='block5_conv1',\n",
    "                                                   activation='relu',\n",
    "                                                   padding='same')\n",
    "        self.block5_conv2 = tf.keras.layers.Conv2D(1024, (3, 3),\n",
    "                                                   name='block5_conv2',\n",
    "                                                   padding='same')\n",
    "        self.block5_bn = tf.keras.layers.BatchNormalization()\n",
    "        self.block5_act = tf.keras.layers.ReLU()\n",
    "        self.block5_dropout = tf.keras.layers.Dropout(0.5)\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        z1 = self.block1_conv1(x)\n",
    "        z1 = self.block1_conv2(z1)\n",
    "        z1 = self.block1_bn(z1)\n",
    "        z1 = self.block1_act(z1)\n",
    "        z1_pool = self.block1_pool(z1)\n",
    "\n",
    "        z2 = self.block2_conv1(z1_pool)\n",
    "        z2 = self.block2_conv2(z2)\n",
    "        z2 = self.block2_bn(z2)\n",
    "        z2 = self.block2_act(z2)\n",
    "        z2_pool = self.block2_pool(z2)\n",
    "\n",
    "        z3 = self.block3_conv1(z2_pool)\n",
    "        z3 = self.block3_conv2(z3)\n",
    "        z3 = self.block3_bn(z3)\n",
    "        z3 = self.block3_act(z3)\n",
    "        z3_pool = self.block3_pool(z3)\n",
    "\n",
    "        z4 = self.block4_conv1(z3_pool)\n",
    "        z4 = self.block4_conv2(z4)\n",
    "        z4 = self.block4_bn(z4)\n",
    "        z4 = self.block4_act(z4)\n",
    "        z4_dropout = self.block4_dropout(z4)\n",
    "        z4_pool = self.block4_pool(z4_dropout)\n",
    "\n",
    "        z5 = self.block5_conv1(z4_pool)\n",
    "        z5 = self.block5_conv2(z5)\n",
    "        z5 = self.block5_bn(z5)\n",
    "        z5 = self.block5_act(z5)\n",
    "        z5_dropout = self.block5_dropout(z5)\n",
    "\n",
    "        return z1, z2, z3, z4_dropout, z5_dropout\n",
    "\n",
    "\n",
    "class Decoder(Model):\n",
    "    def __init__(self, class_num):\n",
    "        super().__init__()\n",
    "        # Network\n",
    "        self.block6_up = tf.keras.layers.UpSampling2D(size=(2, 2))\n",
    "        self.block6_conv1 = tf.keras.layers.Conv2D(512, (2, 2),\n",
    "                                                   name='block6_conv1',\n",
    "                                                   activation='relu',\n",
    "                                                   padding='same')\n",
    "        self.block6_conv2 = tf.keras.layers.Conv2D(512, (3, 3),\n",
    "                                                   name='block6_conv2',\n",
    "                                                   activation='relu',\n",
    "                                                   padding='same')\n",
    "        self.block6_conv3 = tf.keras.layers.Conv2D(512, (3, 3),\n",
    "                                                   name='block6_conv3',\n",
    "                                                   padding='same')\n",
    "        self.block6_bn = tf.keras.layers.BatchNormalization()\n",
    "        self.block6_act = tf.keras.layers.ReLU()\n",
    "\n",
    "        self.block7_up = tf.keras.layers.UpSampling2D(size=(2, 2))\n",
    "        self.block7_conv1 = tf.keras.layers.Conv2D(256, (2, 2),\n",
    "                                                   name='block7_conv1',\n",
    "                                                   activation='relu',\n",
    "                                                   padding='same')\n",
    "        self.block7_conv2 = tf.keras.layers.Conv2D(256, (3, 3),\n",
    "                                                   name='block7_conv2',\n",
    "                                                   activation='relu',\n",
    "                                                   padding='same')\n",
    "        self.block7_conv3 = tf.keras.layers.Conv2D(256, (3, 3),\n",
    "                                                   name='block7_conv3',\n",
    "                                                   padding='same')\n",
    "        self.block7_bn = tf.keras.layers.BatchNormalization()\n",
    "        self.block7_act = tf.keras.layers.ReLU()\n",
    "\n",
    "        self.block8_up = tf.keras.layers.UpSampling2D(size=(2, 2))\n",
    "        self.block8_conv1 = tf.keras.layers.Conv2D(128, (2, 2),\n",
    "                                                   name='block8_conv1',\n",
    "                                                   activation='relu',\n",
    "                                                   padding='same')\n",
    "        self.block8_conv2 = tf.keras.layers.Conv2D(128, (3, 3),\n",
    "                                                   name='block8_conv2',\n",
    "                                                   activation='relu',\n",
    "                                                   padding='same')\n",
    "        self.block8_conv3 = tf.keras.layers.Conv2D(128, (3, 3),\n",
    "                                                   name='block8_conv3',\n",
    "                                                   padding='same')\n",
    "        self.block8_bn = tf.keras.layers.BatchNormalization()\n",
    "        self.block8_act = tf.keras.layers.ReLU()\n",
    "\n",
    "        self.block9_up = tf.keras.layers.UpSampling2D(size=(2, 2))\n",
    "        self.block9_conv1 = tf.keras.layers.Conv2D(64, (2, 2),\n",
    "                                                   name='block9_conv1',\n",
    "                                                   activation='relu',\n",
    "                                                   padding='same')\n",
    "        self.block9_conv2 = tf.keras.layers.Conv2D(64, (3, 3),\n",
    "                                                   name='block9_conv2',\n",
    "                                                   activation='relu',\n",
    "                                                   padding='same')\n",
    "        self.block9_conv3 = tf.keras.layers.Conv2D(64, (3, 3),\n",
    "                                                   name='block9_conv3',\n",
    "                                                   padding='same')\n",
    "        self.block9_bn = tf.keras.layers.BatchNormalization()\n",
    "        self.block9_act = tf.keras.layers.ReLU()\n",
    "        self.output_conv = tf.keras.layers.Conv2D(class_num,\n",
    "                                                  (1, 1), name='output_conv',\n",
    "                                                  activation='sigmoid')\n",
    "\n",
    "    def call(self, z1, z2, z3, z4_dropout, z5_dropout):\n",
    "        z6_up = self.block6_up(z5_dropout)\n",
    "        z6 = self.block6_conv1(z6_up)\n",
    "        z6 = tf.keras.layers.concatenate([z4_dropout, z6], axis=3)\n",
    "        z6 = self.block6_conv2(z6)\n",
    "        z6 = self.block6_conv3(z6)\n",
    "        z6 = self.block6_bn(z6)\n",
    "        z6 = self.block6_act(z6)\n",
    "\n",
    "        z7_up = self.block7_up(z6)\n",
    "        z7 = self.block7_conv1(z7_up)\n",
    "        z7 = tf.keras.layers.concatenate([z3, z7], axis=3)\n",
    "        z7 = self.block7_conv2(z7)\n",
    "        z7 = self.block7_conv3(z7)\n",
    "        z7 = self.block7_bn(z7)\n",
    "        z7 = self.block7_act(z7)\n",
    "\n",
    "        z8_up = self.block8_up(z7)\n",
    "        z8 = self.block8_conv1(z8_up)\n",
    "        z8 = tf.keras.layers.concatenate([z2, z8], axis=3)\n",
    "        z8 = self.block8_conv2(z8)\n",
    "        z8 = self.block8_conv3(z8)\n",
    "        z8 = self.block8_bn(z8)\n",
    "        z8 = self.block8_act(z8)\n",
    "\n",
    "        z9_up = self.block9_up(z8)\n",
    "        z9 = self.block9_conv1(z9_up)\n",
    "        z9 = tf.keras.layers.concatenate([z1, z9], axis=3)\n",
    "        z9 = self.block9_conv2(z9)\n",
    "        z9 = self.block9_conv3(z9)\n",
    "        z9 = self.block9_bn(z9)\n",
    "        z9 = self.block9_act(z9)\n",
    "        y = self.output_conv(z9)\n",
    "\n",
    "        return y\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LRsjdZuEnZfA"
   },
   "source": [
    "最後のレイヤーのフィルタ数は `output_channels` の数に設定されています。これはクラス当たり 1 つの出力チャンネルとなります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j0DGH_4T0VYn"
   },
   "source": [
    "## モデルをトレーニングする\n",
    "\n",
    "では、後は、モデルををコンパイルしてトレーニングするだけです。\n",
    "\n",
    "これはマルチクラスの分類問題であり、ラベルがクラスごとのピクセルのスコアのベクトルではなくスカラー整数であるため、`tf.keras.losses.CategoricalCrossentropy` 損失関数を使用して、`from_logits` を `True` に設定します。\n",
    "\n",
    "推論を実行すると、ピクセルに割り当てられたラベルが最も高い値を持つチャンネルです。これは、`create_mask` 関数の作用です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-09T02:10:27.546125Z",
     "iopub.status.busy": "2022-08-09T02:10:27.545680Z",
     "iopub.status.idle": "2022-08-09T02:10:28.115166Z",
     "shell.execute_reply": "2022-08-09T02:10:28.114394Z"
    },
    "id": "6he36HK5uKAc"
   },
   "outputs": [],
   "source": [
    "OUTPUT_CLASSES = 3\n",
    "\n",
    "# model = unet_model(output_channels=OUTPUT_CLASSES)\n",
    "model = UNet(OUTPUT_CLASSES)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.00001),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "                  from_logits=False),\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tc3MiEO2twLS"
   },
   "source": [
    "トレーニングする前に、モデルが何を予測するかを試してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-09T02:10:28.193567Z",
     "iopub.status.busy": "2022-08-09T02:10:28.193142Z",
     "iopub.status.idle": "2022-08-09T02:10:28.197818Z",
     "shell.execute_reply": "2022-08-09T02:10:28.197160Z"
    },
    "id": "UwvIKLZPtxV_"
   },
   "outputs": [],
   "source": [
    "def create_mask(pred_mask):\n",
    "  print(pred_mask.shape)\n",
    "  pred_mask = tf.argmax(pred_mask, axis=-1)\n",
    "  pred_mask = pred_mask[..., tf.newaxis]\n",
    "  return pred_mask[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-09T02:10:28.201137Z",
     "iopub.status.busy": "2022-08-09T02:10:28.200526Z",
     "iopub.status.idle": "2022-08-09T02:10:28.205328Z",
     "shell.execute_reply": "2022-08-09T02:10:28.204732Z"
    },
    "id": "YLNsrynNtx4d"
   },
   "outputs": [],
   "source": [
    "def show_predictions(dataset=None, num=1):\n",
    "  if dataset:\n",
    "    for image, mask in dataset.take(num):\n",
    "      pred_mask = model.predict(image)\n",
    "      display([image[0], mask[0], create_mask(pred_mask)])\n",
    "  else:\n",
    "    display([sample_image, sample_mask,\n",
    "             create_mask(model.predict(sample_image[tf.newaxis, ...]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-09T02:10:28.208703Z",
     "iopub.status.busy": "2022-08-09T02:10:28.208208Z",
     "iopub.status.idle": "2022-08-09T02:10:30.564302Z",
     "shell.execute_reply": "2022-08-09T02:10:30.563648Z"
    },
    "id": "X_1CC0T4dho3"
   },
   "outputs": [],
   "source": [
    "show_predictions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "22AyVYWQdkgk"
   },
   "source": [
    "以下に定義されるコールバックは、トレーニング中にモデルがどのように改善するかを観測するために使用されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-09T02:10:30.571730Z",
     "iopub.status.busy": "2022-08-09T02:10:30.571176Z",
     "iopub.status.idle": "2022-08-09T02:10:30.575379Z",
     "shell.execute_reply": "2022-08-09T02:10:30.574769Z"
    },
    "id": "wHrHsqijdmL6"
   },
   "outputs": [],
   "source": [
    "class DisplayCallback(tf.keras.callbacks.Callback):\n",
    "  def on_epoch_end(self, epoch, logs=None):\n",
    "    # clear_output(wait=True)\n",
    "    show_predictions()\n",
    "    print ('\\nSample Prediction after epoch {}\\n'.format(epoch+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-09T02:10:30.578866Z",
     "iopub.status.busy": "2022-08-09T02:10:30.578360Z",
     "iopub.status.idle": "2022-08-09T02:12:30.388035Z",
     "shell.execute_reply": "2022-08-09T02:12:30.387324Z"
    },
    "id": "StKDH_B9t4SD",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "VAL_SUBSPLITS = 5\n",
    "VALIDATION_STEPS = info.splits['test'].num_examples//BATCH_SIZE//VAL_SUBSPLITS\n",
    "\n",
    "model_history = model.fit(train_batches, \n",
    "                          epochs=EPOCHS,\n",
    "                          steps_per_epoch=STEPS_PER_EPOCH,\n",
    "                          validation_steps=VALIDATION_STEPS,\n",
    "                          validation_data=test_batches,\n",
    "                          callbacks=[DisplayCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-09T02:12:30.392294Z",
     "iopub.status.busy": "2022-08-09T02:12:30.391730Z",
     "iopub.status.idle": "2022-08-09T02:12:30.506611Z",
     "shell.execute_reply": "2022-08-09T02:12:30.505732Z"
    },
    "id": "P_mu0SAbt40Q"
   },
   "outputs": [],
   "source": [
    "loss = model_history.history['loss']\n",
    "val_loss = model_history.history['val_loss']\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(model_history.epoch, loss, 'r', label='Training loss')\n",
    "plt.plot(model_history.epoch, val_loss, 'bo', label='Validation loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss Value')\n",
    "plt.ylim([0, 1])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "acc = model_history.history['accuracy']\n",
    "val_acc = model_history.history['val_accuracy']\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(model_history.epoch, acc, 'r', label='Training accuracy')\n",
    "plt.plot(range(len(val_acc)), val_acc, 'bo', label='Validation accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy Value')\n",
    "plt.ylim([0, 1])\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "unP3cnxo_N72"
   },
   "source": [
    "## 予測する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7BVXldSo-0mW"
   },
   "source": [
    "いくつか予測を行ってみましょう。時間の節約重視の場合はエポック数を少なくしますが、高精度の結果重視の場合はエポック数を増やして設定します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-09T02:12:30.510184Z",
     "iopub.status.busy": "2022-08-09T02:12:30.509706Z",
     "iopub.status.idle": "2022-08-09T02:12:31.965346Z",
     "shell.execute_reply": "2022-08-09T02:12:31.964682Z"
    },
    "id": "ikrzoG24qwf5"
   },
   "outputs": [],
   "source": [
    "show_predictions(test_batches, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QAwvlgSNoK3o"
   },
   "source": [
    "## オプション: 不均衡なクラスとクラスの重み"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eqtFPqqu2kxP"
   },
   "source": [
    "セマンティックセグメンテーションデータセットは非常に不均衡であり、特定のクラスピクセルが他のクラスに比べて画像の内側寄りに存在する可能性があります。セグメンテーションの問題はピクセル単位の分類問題として対応することができるため、不均衡性を考慮して損失関数を重み付けすることで、不均衡の問題に対処することができます。単純かつエレガントにこの問題に取り組むことができます。詳細は、[不均衡なデータでの分類](../structured_data/imbalanced_data.ipynb)のチュートリアルをご覧ください。\n",
    "\n",
    "[あいまいさを回避](https://github.com/keras-team/keras/issues/3653#issuecomment-243939748)するために、`Model.fit` は 3 次元以上の入力の `class_weight` 引数をサポートしていません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-09T02:12:31.969491Z",
     "iopub.status.busy": "2022-08-09T02:12:31.968907Z",
     "iopub.status.idle": "2022-08-09T02:12:31.978110Z",
     "shell.execute_reply": "2022-08-09T02:12:31.977416Z"
    },
    "id": "aHt90UEQsZDn"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  model_history = model.fit(train_batches, epochs=EPOCHS,\n",
    "                            steps_per_epoch=STEPS_PER_EPOCH,\n",
    "                            class_weight = {0:2.0, 1:2.0, 2:1.0})\n",
    "  assert False\n",
    "except Exception as e:\n",
    "  print(f\"Expected {type(e).__name__}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "brbhYODCsvbe"
   },
   "source": [
    "そのため、この場合、自分で重み付けを実装する必要があります。これにはサンプルの重み付けを使用します。`Model.fit` は `(data, label)` ペアのほかに `(data, label, sample_weight)` トリプレットも受け入れます。\n",
    "\n",
    "`Model.fit` は `sample_weight` を損失とメトリクスに伝搬しますが、`sample_weight` 引数も受け入れます。サンプル重みは、縮小ステップの前にサンプル値で乗算されます。次に例を示します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-09T02:12:31.981273Z",
     "iopub.status.busy": "2022-08-09T02:12:31.980727Z",
     "iopub.status.idle": "2022-08-09T02:12:31.992856Z",
     "shell.execute_reply": "2022-08-09T02:12:31.992292Z"
    },
    "id": "EmHtImJn5Kk-"
   },
   "outputs": [],
   "source": [
    "label = [0,0]\n",
    "prediction = [[-3., 0], [-3, 0]] \n",
    "sample_weight = [1, 10] \n",
    "\n",
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
    "                                               reduction=tf.losses.Reduction.NONE)\n",
    "loss(label, prediction, sample_weight).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gbwo3DZ-9TxM"
   },
   "source": [
    "つまり、このチュートリアルのサンプル重みを作るには、`(data, label)` ペアを取って `(data, label, sample_weight)` トリプルを返す関数が必要となります。`sample_weight` は各ピクセルのクラス重みを含む 1-channel の画像です。\n",
    "\n",
    "実装を可能な限り単純にするために、ラベルを`class_weight` リストのインデックスとして使用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-09T02:12:31.996236Z",
     "iopub.status.busy": "2022-08-09T02:12:31.995742Z",
     "iopub.status.idle": "2022-08-09T02:12:31.999775Z",
     "shell.execute_reply": "2022-08-09T02:12:31.999240Z"
    },
    "id": "DlG-n2Ugo8Jc"
   },
   "outputs": [],
   "source": [
    "def add_sample_weights(image, label):\n",
    "  # The weights for each class, with the constraint that:\n",
    "  #     sum(class_weights) == 1.0\n",
    "  class_weights = tf.constant([2.0, 2.0, 1.0])\n",
    "  class_weights = class_weights/tf.reduce_sum(class_weights)\n",
    "\n",
    "  # Create an image of `sample_weights` by using the label at each pixel as an \n",
    "  # index into the `class weights` .\n",
    "  sample_weights = tf.gather(class_weights, indices=tf.cast(label, tf.int32))\n",
    "\n",
    "  return image, label, sample_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hLH_NvH2UrXU"
   },
   "source": [
    "この結果、データセットの各要素には、3 つの画像が含まれます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-09T02:12:32.002729Z",
     "iopub.status.busy": "2022-08-09T02:12:32.002295Z",
     "iopub.status.idle": "2022-08-09T02:12:32.053546Z",
     "shell.execute_reply": "2022-08-09T02:12:32.052912Z"
    },
    "id": "SE_ezRSFRCnE"
   },
   "outputs": [],
   "source": [
    "train_batches.map(add_sample_weights).element_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yc-EpIzaRbSL"
   },
   "source": [
    "これで、この重み付けが付けられたデータセットでモデルをトレーニングできるようになりました。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-09T02:12:32.056878Z",
     "iopub.status.busy": "2022-08-09T02:12:32.056223Z",
     "iopub.status.idle": "2022-08-09T02:12:32.485808Z",
     "shell.execute_reply": "2022-08-09T02:12:32.484918Z"
    },
    "id": "QDWipedAoOQe"
   },
   "outputs": [],
   "source": [
    "weighted_model = unet_model(OUTPUT_CLASSES)\n",
    "weighted_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-09T02:12:32.490054Z",
     "iopub.status.busy": "2022-08-09T02:12:32.489404Z",
     "iopub.status.idle": "2022-08-09T02:12:36.359074Z",
     "shell.execute_reply": "2022-08-09T02:12:36.358379Z"
    },
    "id": "btEFKc1xodGR"
   },
   "outputs": [],
   "source": [
    "weighted_model.fit(\n",
    "    train_batches.map(add_sample_weights),\n",
    "    epochs=1,\n",
    "    steps_per_epoch=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R24tahEqmSCk"
   },
   "source": [
    "## 次のステップ\n",
    "\n",
    "これで画像セグメンテーションとは何か、それがどのように機能するかについての知識が得られたはずです。このチュートリアルは、異なる中間レイヤー出力や、異なる事前トレーニング済みモデルでも試すことができます。また、Kaggle がホストしている [Carvana](https://www.kaggle.com/c/carvana-image-masking-challenge/overview) 画像マスキングチャレンジに挑戦してみることもお勧めです。\n",
    "\n",
    "[Tensorflow Object Detection API](https://github.com/tensorflow/models/blob/master/research/object_detection/README.md) を参照して、独自のデータで再トレーニング可能な別のモデルを確認するのも良いでしょう。トレーニング済みのモデルは、[TensorFlow Hub](https://www.tensorflow.org/hub/tutorials/tf2_object_detection#optional) にあります。"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "segmentation.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
